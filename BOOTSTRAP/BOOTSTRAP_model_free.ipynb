{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN1e4QoH6pVELAOTOxI4lEu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"-5nsntvSNkZc","executionInfo":{"status":"ok","timestamp":1677071784477,"user_tz":-60,"elapsed":7,"user":{"displayName":"Benjamin “Benjyhy” Ramet","userId":"03636067544966111940"}}},"outputs":[],"source":["import numpy as np\n","import gym"]},{"cell_type":"code","source":["env = gym.make(\"FrozenLake-v1\")\n","n_observations = env.observation_space.n\n","n_actions = env.action_space.n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"li5QHReDNo9o","executionInfo":{"status":"ok","timestamp":1677071784477,"user_tz":-60,"elapsed":6,"user":{"displayName":"Benjamin “Benjyhy” Ramet","userId":"03636067544966111940"}},"outputId":"ea1cf890-ddba-4bb9-b4c9-dc38b57decc1"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"code","source":["#Initialize the Q-table to 0\n","Q_table = np.zeros((n_observations,n_actions))\n","print(Q_table)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VKJE4ME1Nsgg","executionInfo":{"status":"ok","timestamp":1677071785914,"user_tz":-60,"elapsed":1442,"user":{"displayName":"Benjamin “Benjyhy” Ramet","userId":"03636067544966111940"}},"outputId":"73ae92b1-9af4-4dfe-f472-271fc3c3f7e5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"]}]},{"cell_type":"code","source":["#number of episode we will run\n","n_episodes = 10000\n","\n","#maximum of iteration per episode\n","max_iter_episode = 100\n","\n","#initialize the exploration probability to 1\n","exploration_proba = 1\n","\n","#exploartion decreasing decay for exponential decreasing\n","exploration_decreasing_decay = 0.001\n","\n","# minimum of exploration proba\n","min_exploration_proba = 0.01\n","\n","#discounted factor\n","gamma = 0.99\n","\n","#learning rate\n","lr = 0.1"],"metadata":{"id":"5jDtLpUcNuiv","executionInfo":{"status":"ok","timestamp":1677072282603,"user_tz":-60,"elapsed":231,"user":{"displayName":"Benjamin “Benjyhy” Ramet","userId":"03636067544966111940"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["total_rewards_episode = list()"],"metadata":{"id":"3Byc1yynNwht","executionInfo":{"status":"ok","timestamp":1677072284676,"user_tz":-60,"elapsed":2,"user":{"displayName":"Benjamin “Benjyhy” Ramet","userId":"03636067544966111940"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["#we iterate over episodes\n","for e in range(n_episodes):\n","    #we initialize the first state of the episode\n","    current_state = env.reset()\n","    done = False\n","    \n","    #sum the rewards that the agent gets from the environment\n","    total_episode_reward = 0\n","    \n","    for i in range(max_iter_episode): \n","        # we sample a float from a uniform distribution over 0 and 1\n","        # if the sampled flaot is less than the exploration proba\n","        #     the agent selects arandom action\n","        # else\n","        #     he exploits his knowledge using the bellman equation \n","        \n","        if np.random.uniform(0,1) < exploration_proba:\n","            action = env.action_space.sample()\n","        else:\n","            action = np.argmax(Q_table[current_state,:])\n","        \n","        # The environment runs the chosen action and returns\n","        # the next state, a reward and true if the epiosed is ended.\n","        next_state, reward, done, _ = env.step(action)\n","        \n","        # We update our Q-table using the Q-learning iteration\n","        Q_table[current_state, action] = (1-lr) * Q_table[current_state, action] +lr*(reward + gamma*max(Q_table[next_state,:]))\n","        total_episode_reward += reward\n","        # If the episode is finished, we leave the for loop\n","        if done:\n","            break\n","        current_state = next_state\n","    #We update the exploration proba using exponential decay formula \n","    exploration_proba = max(min_exploration_proba, np.exp(-exploration_decreasing_decay*e))\n","    total_rewards_episode.append(total_episode_reward)"],"metadata":{"id":"FUwbO0H1N0PK","executionInfo":{"status":"ok","timestamp":1677072301208,"user_tz":-60,"elapsed":13823,"user":{"displayName":"Benjamin “Benjyhy” Ramet","userId":"03636067544966111940"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["print(\"Mean reward per thousand episodes\")\n","for i in range(10):\n","    print(f\"{(i+1)*1000}: mean espiode reward: {np.mean(total_rewards_episode[1000*i:1000*(i+1)])}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2dC3QxPuN4Ml","executionInfo":{"status":"ok","timestamp":1677072302879,"user_tz":-60,"elapsed":5,"user":{"displayName":"Benjamin “Benjyhy” Ramet","userId":"03636067544966111940"}},"outputId":"3f154080-d557-4141-9974-0bbd3710a151"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean reward per thousand episodes\n","1000: mean espiode reward: 0.054\n","2000: mean espiode reward: 0.232\n","3000: mean espiode reward: 0.446\n","4000: mean espiode reward: 0.581\n","5000: mean espiode reward: 0.67\n","6000: mean espiode reward: 0.702\n","7000: mean espiode reward: 0.703\n","8000: mean espiode reward: 0.683\n","9000: mean espiode reward: 0.675\n","10000: mean espiode reward: 0.685\n"]}]},{"cell_type":"code","source":["Q_table"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r1XxrOoZO6Eb","executionInfo":{"status":"ok","timestamp":1677072302880,"user_tz":-60,"elapsed":4,"user":{"displayName":"Benjamin “Benjyhy” Ramet","userId":"03636067544966111940"}},"outputId":"04c8218b-ff4a-4385-dfae-90dc1fb491d4"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.57682487, 0.54200049, 0.51518813, 0.51374319],\n","       [0.24928177, 0.34827999, 0.34262678, 0.53005142],\n","       [0.43647244, 0.42760149, 0.42414091, 0.48627636],\n","       [0.28127904, 0.32058023, 0.34538009, 0.47020522],\n","       [0.60161581, 0.35210911, 0.23750153, 0.36754199],\n","       [0.        , 0.        , 0.        , 0.        ],\n","       [0.32509104, 0.21105555, 0.21668728, 0.10291924],\n","       [0.        , 0.        , 0.        , 0.        ],\n","       [0.45568092, 0.37969775, 0.39296602, 0.6662461 ],\n","       [0.42630769, 0.71545623, 0.41826643, 0.46110979],\n","       [0.69127231, 0.34514193, 0.3788338 , 0.33627272],\n","       [0.        , 0.        , 0.        , 0.        ],\n","       [0.        , 0.        , 0.        , 0.        ],\n","       [0.30032979, 0.63590317, 0.78218105, 0.45432522],\n","       [0.71472409, 0.89826561, 0.78559449, 0.74995767],\n","       [0.        , 0.        , 0.        , 0.        ]])"]},"metadata":{},"execution_count":32}]}]}